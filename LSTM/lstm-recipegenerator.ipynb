{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7327010,"sourceType":"datasetVersion","datasetId":4252793}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  LSTM on Using Eucledian Recipe Data","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we'll walk through the steps required to train your own LSTM on the recipes dataset\n\n# LSTM Recipe Generation Notebook Overview\n\n## Overview\n\nThis Jupyter Notebook demonstrates the process of training a Long Short-Term Memory (LSTM) model on a recipe dataset for generating new recipes. The model is built using TensorFlow and leverages the power of recurrent neural networks to learn patterns and structures in the provided recipes.\n\n## Sections\n\n1. **Parameters Setup**\n   - Define essential parameters such as vocabulary size, maximum sequence length, embedding dimensions, LSTM units, etc.\n\n2. **Load the Data**\n   - Load the recipe dataset from a JSON file, filter and preprocess the data.\n\n3. **Tokenize the Data**\n   - Tokenize the recipes by padding punctuation and convert them into a format suitable for training.\n\n4. **Create the Training Set**\n   - Prepare the input and output sequences for training the LSTM model.\n\n5. **Build the LSTM Model**\n   - Define the architecture of the LSTM model using TensorFlow's Keras API.\n\n6. **Train the LSTM Model**\n   - Compile and train the LSTM model on the prepared dataset. Checkpoints and TensorBoard are utilized for monitoring.\n\n7. **Generate Text using the LSTM**\n   - Implement a TextGenerator callback to generate text during training. Use the trained model to generate recipes based on specified prompts.\n\n8. **Print Probability Analysis**\n   - Evaluate the probabilities of predicted words for different prompts and temperatures.\n\n9. **Save the Model**\n   - Save the final trained model for future use.\n\n10. **Results and Usage**\n    - Discuss the generated results, provide instructions for model usage, and showcase generated recipes.\n\n## Usage\n\n1. Clone the repository and navigate to the project directory.\n\n2. Install the required dependencies using `pip install -r requirements.txt`.\n\n3. Download the recipe dataset and place it in the `/data` directory.\n\n4. Run the Jupyter Notebook: `jupyter notebook LSTM_Recipe_Generation.ipynb`.\n\n5. Experiment with different parameters, prompts, and temperatures to generate diverse recipes.\n\n\n","metadata":{}},{"cell_type":"code","source":"\n\nimport numpy as np\nimport json\nimport re\nimport string\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks, losses","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:25.687567Z","iopub.execute_input":"2024-01-03T02:45:25.687972Z","iopub.status.idle":"2024-01-03T02:45:30.365203Z","shell.execute_reply.started":"2024-01-03T02:45:25.687939Z","shell.execute_reply":"2024-01-03T02:45:30.363689Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 0. Parameters <a name=\"parameters\"></a>\n\n1. **VOCAB_SIZE (10000):**\n   - The size of the vocabulary, which represents the total number of unique words in your dataset. It is often set to a specific number to limit the vocabulary size and make the training process more manageable.\n\n2. **MAX_LEN (200):**\n   - Maximum sequence length. It defines the maximum number of tokens (words or characters) in each input sequence. Sequences longer than this length will be truncated, and sequences shorter than this length will be padded.\n\n3. **EMBEDDING_DIM (100):**\n   - The dimensionality of the word embeddings. Each word in the vocabulary will be represented as a dense vector of this size. Embeddings capture semantic relationships between words.\n\n4. **N_UNITS (128):**\n   - The number of LSTM units or cells in the LSTM layer. LSTM units are responsible for learning and capturing sequential patterns in the input data.\n\n5. **VALIDATION_SPLIT (0.2):**\n   - The fraction of the dataset that will be used for validation during training. In this case, 20% of the data will be reserved for validation, helping to monitor the model's performance on data it has not seen during training.\n\n6. **SEED (42):**\n   - A random seed for reproducibility. Setting a seed ensures that the random initialization of weights in the model and shuffling of the dataset are consistent across runs, making experiments reproducible.\n\n7. **LOAD_MODEL (False):**\n   - A boolean flag indicating whether to load a pre-trained model. If set to `True`, the notebook will attempt to load a saved model instead of training a new one.\n\n8. **BATCH_SIZE (32):**\n   - The number of samples used in each iteration during training. It defines how many training examples are processed together before updating the model's weights. A smaller batch size may lead to a more stable training process.\n\n9. **EPOCHS (3):**\n   - The number of epochs for training the model. An epoch is one complete pass through the entire training dataset. Training for multiple epochs allows the model to learn from the data iteratively.\n","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = 10000\nMAX_LEN = 200\nEMBEDDING_DIM = 100\nN_UNITS = 128\nVALIDATION_SPLIT = 0.2\nSEED = 42\nLOAD_MODEL = False\nBATCH_SIZE = 32\nEPOCHS = 3","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:30.367733Z","iopub.execute_input":"2024-01-03T02:45:30.368478Z","iopub.status.idle":"2024-01-03T02:45:30.375829Z","shell.execute_reply.started":"2024-01-03T02:45:30.368436Z","shell.execute_reply":"2024-01-03T02:45:30.374477Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 1. Load the data <a name=\"load\"></a>","metadata":{}},{"cell_type":"code","source":"# Load the full dataset\nwith open(\"/kaggle/input/recipe/full_format_recipes.json\") as json_data:\n    recipe_data = json.load(json_data)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T02:45:30.378029Z","iopub.execute_input":"2024-01-03T02:45:30.379124Z","iopub.status.idle":"2024-01-03T02:45:31.048114Z","shell.execute_reply.started":"2024-01-03T02:45:30.379061Z","shell.execute_reply":"2024-01-03T02:45:31.046790Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Filter the dataset\nfiltered_data = [\n    \"Recipe for \" + x[\"title\"] + \" | \" + \" \".join(x[\"directions\"])\n    for x in recipe_data\n    if \"title\" in x\n    and x[\"title\"] is not None\n    and \"directions\" in x\n    and x[\"directions\"] is not None\n]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T02:45:31.049669Z","iopub.execute_input":"2024-01-03T02:45:31.050086Z","iopub.status.idle":"2024-01-03T02:45:31.115670Z","shell.execute_reply.started":"2024-01-03T02:45:31.050049Z","shell.execute_reply":"2024-01-03T02:45:31.114314Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Count the recipes\nn_recipes = len(filtered_data)\nprint(f\"{n_recipes} recipes loaded\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T02:45:31.119429Z","iopub.execute_input":"2024-01-03T02:45:31.119842Z","iopub.status.idle":"2024-01-03T02:45:31.126067Z","shell.execute_reply.started":"2024-01-03T02:45:31.119806Z","shell.execute_reply":"2024-01-03T02:45:31.124701Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"20111 recipes loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"example = filtered_data[9]\nprint(example)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:31.127693Z","iopub.execute_input":"2024-01-03T02:45:31.128152Z","iopub.status.idle":"2024-01-03T02:45:31.141271Z","shell.execute_reply.started":"2024-01-03T02:45:31.128113Z","shell.execute_reply":"2024-01-03T02:45:31.139786Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas  | Chop enough parsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan, covered, 5 minutes. Meanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute. Strain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve. Season with salt and pepper. Set bowl in an ice bath and cool to room temperature, stirring. Toss ham with reserved parsley and divide among jars. Pour gelatin on top and chill until set, at least 1 hour. Whisk together mayonnaise, mustard, vinegar, 1/4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery, cornichons, and potatoes. Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and 1/4 teaspoon salt in a food processor to a coarse mash. Layer peas, then potato salad, over ham.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. Tokenise the data","metadata":{}},{"cell_type":"code","source":"# Pad the punctuation, to treat them as separate 'words'\ndef pad_punctuation(s):\n    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n    s = re.sub(\" +\", \" \", s)\n    return s\n\n\ntext_data = [pad_punctuation(x) for x in filtered_data]","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T02:45:31.143137Z","iopub.execute_input":"2024-01-03T02:45:31.143663Z","iopub.status.idle":"2024-01-03T02:45:34.462145Z","shell.execute_reply.started":"2024-01-03T02:45:31.143614Z","shell.execute_reply":"2024-01-03T02:45:34.461206Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Display an example of a recipe\nexample_data = text_data[9]\nexample_data","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:34.463738Z","iopub.execute_input":"2024-01-03T02:45:34.464138Z","iopub.status.idle":"2024-01-03T02:45:34.471739Z","shell.execute_reply.started":"2024-01-03T02:45:34.464105Z","shell.execute_reply":"2024-01-03T02:45:34.470898Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas | Chop enough parsley leaves to measure 1 tablespoon ; reserve . Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan , covered , 5 minutes . Meanwhile , sprinkle gelatin over water in a medium bowl and let soften 1 minute . Strain broth through a fine - mesh sieve into bowl with gelatin and stir to dissolve . Season with salt and pepper . Set bowl in an ice bath and cool to room temperature , stirring . Toss ham with reserved parsley and divide among jars . Pour gelatin on top and chill until set , at least 1 hour . Whisk together mayonnaise , mustard , vinegar , 1 / 4 teaspoon salt , and 1 / 4 teaspoon pepper in a large bowl . Stir in celery , cornichons , and potatoes . Pulse peas with marjoram , oil , 1 / 2 teaspoon pepper , and 1 / 4 teaspoon salt in a food processor to a coarse mash . Layer peas , then potato salad , over ham . '"},"metadata":{}}]},{"cell_type":"code","source":"# Convert to a Tensorflow Dataset\ntext_ds = (\n    tf.data.Dataset.from_tensor_slices(text_data)\n    .batch(BATCH_SIZE)\n    .shuffle(1000)\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T02:45:34.472883Z","iopub.execute_input":"2024-01-03T02:45:34.473805Z","iopub.status.idle":"2024-01-03T02:45:34.838476Z","shell.execute_reply.started":"2024-01-03T02:45:34.473771Z","shell.execute_reply":"2024-01-03T02:45:34.837158Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create a vectorisation layer\nvectorize_layer = layers.TextVectorization(\n    standardize=\"lower\",\n    max_tokens=VOCAB_SIZE,\n    output_mode=\"int\",\n    output_sequence_length=MAX_LEN + 1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:34.840373Z","iopub.execute_input":"2024-01-03T02:45:34.840757Z","iopub.status.idle":"2024-01-03T02:45:34.870617Z","shell.execute_reply.started":"2024-01-03T02:45:34.840725Z","shell.execute_reply":"2024-01-03T02:45:34.869035Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Adapt the layer to the training set\nvectorize_layer.adapt(text_ds)\nvocab = vectorize_layer.get_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:34.872329Z","iopub.execute_input":"2024-01-03T02:45:34.872706Z","iopub.status.idle":"2024-01-03T02:45:37.570602Z","shell.execute_reply.started":"2024-01-03T02:45:34.872675Z","shell.execute_reply":"2024-01-03T02:45:37.569287Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Display some token:word mappings\nfor i, word in enumerate(vocab[:10]):\n    print(f\"{i}: {word}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:37.572385Z","iopub.execute_input":"2024-01-03T02:45:37.572851Z","iopub.status.idle":"2024-01-03T02:45:37.580651Z","shell.execute_reply.started":"2024-01-03T02:45:37.572805Z","shell.execute_reply":"2024-01-03T02:45:37.579159Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"0: \n1: [UNK]\n2: .\n3: ,\n4: and\n5: to\n6: in\n7: the\n8: with\n9: a\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display the same example converted to ints\nexample_tokenised = vectorize_layer(example_data)\nprint(example_tokenised.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:37.582226Z","iopub.execute_input":"2024-01-03T02:45:37.582605Z","iopub.status.idle":"2024-01-03T02:45:37.640072Z","shell.execute_reply.started":"2024-01-03T02:45:37.582571Z","shell.execute_reply":"2024-01-03T02:45:37.638802Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[  26   16  557    1    8  298  335  189    4 1054  494   27  332  228\n  235  262    5  594   11  133   22  311    2  332   45  262    4  671\n    4   70    8  171    4   81    6    9   65   80    3  121    3   59\n   12    2  299    3   88  650   20   39    6    9   29   21    4   67\n  529   11  164    2  320  171  102    9  374   13  643  306   25   21\n    8  650    4   42    5  931    2   63    8   24    4   33    2  114\n   21    6  178  181 1245    4   60    5  140  112    3   48    2  117\n  557    8  285  235    4  200  292  980    2  107  650   28   72    4\n  108   10  114    3   57  204   11  172    2   73  110  482    3  298\n    3  190    3   11   23   32  142   24    3    4   11   23   32  142\n   33    6    9   30   21    2   42    6  353    3 3224    3    4  150\n    2  437  494    8 1281    3   37    3   11   23   15  142   33    3\n    4   11   23   32  142   24    6    9  291  188    5    9  412  572\n    2  230  494    3   46  335  189    3   20  557    2    0    0    0\n    0    0    0    0    0]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 3. Create the Training Set","metadata":{}},{"cell_type":"code","source":"# Create the training set of recipes and the same text shifted by one word\ndef prepare_inputs(text):\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y\n\n\ntrain_ds = text_ds.map(prepare_inputs)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:37.645097Z","iopub.execute_input":"2024-01-03T02:45:37.645481Z","iopub.status.idle":"2024-01-03T02:45:37.774491Z","shell.execute_reply.started":"2024-01-03T02:45:37.645447Z","shell.execute_reply":"2024-01-03T02:45:37.772907Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 4. Build the LSTM <a name=\"build\"></a>\n\nInput Layer:\n\ninputs = layers.Input(shape=(None,), dtype=\"int32\"): Defines an input layer for variable-length sequences. It specifies that the input data will be integer-encoded sequences.\nEmbedding Layer:\n\nx = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs): Converts integer-encoded input sequences into dense vectors. Each word in the sequence is represented by a dense vector of size EMBEDDING_DIM.\nLSTM Layer:\n\nx = layers.LSTM(N_UNITS, return_sequences=True)(x): Utilizes an LSTM layer with N_UNITS cells. return_sequences=True is set to return the full sequence of outputs for each input sequence.\nDense Layer (Output):\n\noutputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x): The dense layer with VOCAB_SIZE units and softmax activation. It produces a probability distribution over the vocabulary for each element in the sequence.\nModel Compilation:\n\nlstm = models.Model(inputs, outputs): Creates the LSTM model with the specified input and output layers.\nModel Summary:\n\nlstm.summary(): Displays a summary of the model, including layer names, output shapes, and the number of parameters. This summary is useful for understanding the architecture and ensuring that it matches the intended design.","metadata":{"tags":[]}},{"cell_type":"code","source":"inputs = layers.Input(shape=(None,), dtype=\"int32\")\nx = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\nx = layers.LSTM(N_UNITS, return_sequences=True)(x)\noutputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\nlstm = models.Model(inputs, outputs)\nlstm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:37.776390Z","iopub.execute_input":"2024-01-03T02:45:37.776752Z","iopub.status.idle":"2024-01-03T02:45:38.216823Z","shell.execute_reply.started":"2024-01-03T02:45:37.776722Z","shell.execute_reply":"2024-01-03T02:45:38.215254Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding (Embedding)       (None, None, 100)         1000000   \n                                                                 \n lstm (LSTM)                 (None, None, 128)         117248    \n                                                                 \n dense (Dense)               (None, None, 10000)       1290000   \n                                                                 \n=================================================================\nTotal params: 2407248 (9.18 MB)\nTrainable params: 2407248 (9.18 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"if LOAD_MODEL:\n    # model.load_weights('./models/model')\n    lstm = models.load_model(\"./models/lstm\", compile=False)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T02:45:38.218737Z","iopub.execute_input":"2024-01-03T02:45:38.219257Z","iopub.status.idle":"2024-01-03T02:45:38.226159Z","shell.execute_reply.started":"2024-01-03T02:45:38.219209Z","shell.execute_reply":"2024-01-03T02:45:38.224694Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## 5. Train the LSTM <a name=\"train\"></a>","metadata":{}},{"cell_type":"code","source":"loss_fn = losses.SparseCategoricalCrossentropy()\nlstm.compile(\"adam\", loss_fn)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:38.228139Z","iopub.execute_input":"2024-01-03T02:45:38.228594Z","iopub.status.idle":"2024-01-03T02:45:38.261017Z","shell.execute_reply.started":"2024-01-03T02:45:38.228559Z","shell.execute_reply":"2024-01-03T02:45:38.259704Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Create a TextGenerator checkpoint\nclass TextGenerator(callbacks.Callback):\n    def __init__(self, index_to_word, top_k=10):\n        self.index_to_word = index_to_word\n        self.word_to_index = {\n            word: index for index, word in enumerate(index_to_word)\n        }  # <1>\n\n    def sample_from(self, probs, temperature):  # <2>\n        probs = probs ** (1 / temperature)\n        probs = probs / np.sum(probs)\n        return np.random.choice(len(probs), p=probs), probs\n\n    def generate(self, start_prompt, max_tokens, temperature):\n        start_tokens = [\n            self.word_to_index.get(x, 1) for x in start_prompt.split()\n        ]  # <3>\n        sample_token = None\n        info = []\n        while len(start_tokens) < max_tokens and sample_token != 0:  # <4>\n            x = np.array([start_tokens])\n            y = self.model.predict(x, verbose=0)  # <5>\n            sample_token, probs = self.sample_from(y[0][-1], temperature)  # <6>\n            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n            start_tokens.append(sample_token)  # <7>\n            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n        return info\n\n    def on_epoch_end(self, epoch, logs=None):\n        self.generate(\"recipe for\", max_tokens=100, temperature=1.0)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:38.262436Z","iopub.execute_input":"2024-01-03T02:45:38.262827Z","iopub.status.idle":"2024-01-03T02:45:38.275791Z","shell.execute_reply.started":"2024-01-03T02:45:38.262793Z","shell.execute_reply":"2024-01-03T02:45:38.274532Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Create a model save checkpoint\nmodel_checkpoint_callback = callbacks.ModelCheckpoint(\n    filepath=\"./checkpoint/checkpoint.ckpt\",\n    save_weights_only=True,\n    save_freq=\"epoch\",\n    verbose=0,\n)\n\ntensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n\n# Tokenize starting prompt\ntext_generator = TextGenerator(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T02:45:38.277076Z","iopub.execute_input":"2024-01-03T02:45:38.277475Z","iopub.status.idle":"2024-01-03T02:45:38.300407Z","shell.execute_reply.started":"2024-01-03T02:45:38.277439Z","shell.execute_reply":"2024-01-03T02:45:38.299179Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"lstm.fit(\n    train_ds,\n    epochs=EPOCHS,\n    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T02:45:38.302126Z","iopub.execute_input":"2024-01-03T02:45:38.302513Z","iopub.status.idle":"2024-01-03T03:21:52.682039Z","shell.execute_reply.started":"2024-01-03T02:45:38.302479Z","shell.execute_reply":"2024-01-03T03:21:52.680617Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/3\n629/629 [==============================] - ETA: 0s - loss: 4.4749\ngenerated text:\nrecipe for , 25 of stir preheat vinegar and potato with put feed heavy segments crostini of very a wrap , tent , place measure thyme and sauté minutes . simmer , pat onions . until coarsely nuts the a large meat until beef sides with cups sauce . pepper . bake freshly it . watching pod . simmering 3 . tuna as have if egg tablespoons shoulder . 50 to crusts on scorching \n\n629/629 [==============================] - 688s 1s/step - loss: 4.4749\nEpoch 2/3\n629/629 [==============================] - ETA: 0s - loss: 3.1710\ngenerated text:\nrecipe for honeydew of microwave apple teaspoon arugula zest | beat in salt lemon . place sauce in large layer in skillet over heat heat until vanilla , about 30 minutes . add tomatoes and pepper until stick pie only if sheet bubbles . if desired after chicken , oysters occasionally , until pale brown , cover and preferably onto fragrant , about 10 minutes . stir the sugar , bay , onion , and taste and yolks ; tongs gently gently gently purée to coat . cut center of in paper or adhere . if smooth . jalapeño the\n\n629/629 [==============================] - 689s 1s/step - loss: 3.1710\nEpoch 3/3\n629/629 [==============================] - ETA: 0s - loss: 2.5711\ngenerated text:\nrecipe for eggs chunks with toasted shallot , parsley , ginger , and olive walnuts | place all cups strawberries in large skillet and boil 4 teaspoon 2 teaspoon potato . meanwhile , remove rim of the greens from rhubarb upside . bake it in a large bowl until smooth and reserve flameproof sour - cup oil , for a spoon , 1 / 4 cup and cooked through , about 1 5 minutes . place in 2 tablespoons pepper to 425°f . melt oil in a large large baking dish with generous 1 1 / 2 tablespoons baking cranberries\n\n629/629 [==============================] - 688s 1s/step - loss: 2.5711\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7a412e9c3eb0>"},"metadata":{}}]},{"cell_type":"markdown","source":"**The auto-generated text** appears to be the output of the LSTM (Long Short-Term Memory) model trained on a recipe dataset. The training process involved learning patterns and structures within the provided recipes, enabling the model to generate new text based on a given prompt.\n\n**The results of the auto-generator** show text snippets that resemble recipes. However, the generated text exhibits a balance between coherence and diversity, influenced by the temperature parameter during sampling. Higher temperatures (e.g., 1.0) result in more diverse but potentially less coherent text, while lower temperatures (e.g., 0.2) make the generated text more deterministic and focused.\n\n**The training log** displays the loss values during each epoch of training, indicating how well the model is learning from the dataset. Additionally, the generated examples after each epoch provide insights into the model's evolving capabilities in creating coherent and contextually relevant recipe-like text based on the learned patterns.\n\n**Overall, the auto-generator** demonstrates the model's ability to generate human-like text based on the learned knowledge from the recipe dataset, offering a glimpse into its creative and pattern-recognition capabilities.\n","metadata":{}},{"cell_type":"code","source":"# Save the final model\nlstm.save(\"./models/lstm\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T03:21:52.683430Z","iopub.execute_input":"2024-01-03T03:21:52.683801Z","iopub.status.idle":"2024-01-03T03:21:57.348820Z","shell.execute_reply.started":"2024-01-03T03:21:52.683770Z","shell.execute_reply":"2024-01-03T03:21:57.347257Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## 6. Generate text using the LSTM","metadata":{}},{"cell_type":"code","source":"def print_probs(info, vocab, top_k=5):\n    for i in info:\n        print(f\"\\nPROMPT: {i['prompt']}\")\n        word_probs = i[\"word_probs\"]\n        p_sorted = np.sort(word_probs)[::-1][:top_k]\n        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n        for p, i in zip(p_sorted, i_sorted):\n            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n        print(\"--------\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:21:57.351859Z","iopub.execute_input":"2024-01-03T03:21:57.352847Z","iopub.status.idle":"2024-01-03T03:21:57.361824Z","shell.execute_reply.started":"2024-01-03T03:21:57.352797Z","shell.execute_reply":"2024-01-03T03:21:57.359850Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"info = text_generator.generate(\n    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=1.0\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:21:57.363507Z","iopub.execute_input":"2024-01-03T03:21:57.364032Z","iopub.status.idle":"2024-01-03T03:21:57.538853Z","shell.execute_reply.started":"2024-01-03T03:21:57.363981Z","shell.execute_reply":"2024-01-03T03:21:57.537641Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\ngenerated text:\nrecipe for roasted vegetables | chop 1 / 4 cups\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print_probs(info, vocab)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-01-03T03:21:57.540380Z","iopub.execute_input":"2024-01-03T03:21:57.541219Z","iopub.status.idle":"2024-01-03T03:21:57.553704Z","shell.execute_reply.started":"2024-01-03T03:21:57.541160Z","shell.execute_reply":"2024-01-03T03:21:57.552304Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\nPROMPT: recipe for roasted vegetables | chop 1 /\n2:   \t42.78%\n4:   \t33.32%\n3:   \t7.24%\n8:   \t3.59%\n1:   \t3.33%\n--------\n\n\nPROMPT: recipe for roasted vegetables | chop 1 / 4\n-:   \t53.76%\ncup:   \t19.65%\ninch:   \t4.73%\ncups:   \t4.3%\nteaspoon:   \t2.38%\n--------\n\n","output_type":"stream"}]},{"cell_type":"code","source":"info = text_generator.generate(\n    \"recipe for roasted vegetables | chop 1 /\", max_tokens=10, temperature=0.2\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:21:57.555282Z","iopub.execute_input":"2024-01-03T03:21:57.556687Z","iopub.status.idle":"2024-01-03T03:21:57.723564Z","shell.execute_reply.started":"2024-01-03T03:21:57.556641Z","shell.execute_reply":"2024-01-03T03:21:57.722058Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\ngenerated text:\nrecipe for roasted vegetables | chop 1 / 2 -\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print_probs(info, vocab)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:21:57.726397Z","iopub.execute_input":"2024-01-03T03:21:57.726807Z","iopub.status.idle":"2024-01-03T03:21:57.735830Z","shell.execute_reply.started":"2024-01-03T03:21:57.726773Z","shell.execute_reply":"2024-01-03T03:21:57.734702Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\nPROMPT: recipe for roasted vegetables | chop 1 /\n2:   \t77.71%\n4:   \t22.27%\n3:   \t0.01%\n8:   \t0.0%\n1:   \t0.0%\n--------\n\n\nPROMPT: recipe for roasted vegetables | chop 1 / 2\n-:   \t99.78%\ncup:   \t0.22%\ncups:   \t0.0%\n/:   \t0.0%\ninch:   \t0.0%\n--------\n\n","output_type":"stream"}]},{"cell_type":"code","source":"info = text_generator.generate(\n    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=1.0\n)\nprint_probs(info, vocab)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:21:57.737285Z","iopub.execute_input":"2024-01-03T03:21:57.737932Z","iopub.status.idle":"2024-01-03T03:21:57.825283Z","shell.execute_reply.started":"2024-01-03T03:21:57.737884Z","shell.execute_reply":"2024-01-03T03:21:57.824239Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\ngenerated text:\nrecipe for chocolate ice cream | cook\n\n\nPROMPT: recipe for chocolate ice cream |\npreheat:   \t14.89%\ncombine:   \t10.14%\nin:   \t9.32%\nwhisk:   \t4.49%\nheat:   \t4.31%\n--------\n\n","output_type":"stream"}]},{"cell_type":"code","source":"info = text_generator.generate(\n    \"recipe for chocolate ice cream |\", max_tokens=7, temperature=0.2\n)\nprint_probs(info, vocab)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T03:21:57.826806Z","iopub.execute_input":"2024-01-03T03:21:57.827577Z","iopub.status.idle":"2024-01-03T03:21:57.926184Z","shell.execute_reply.started":"2024-01-03T03:21:57.827532Z","shell.execute_reply":"2024-01-03T03:21:57.924761Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\ngenerated text:\nrecipe for chocolate ice cream | preheat\n\n\nPROMPT: recipe for chocolate ice cream |\npreheat:   \t79.9%\ncombine:   \t11.69%\nin:   \t7.69%\nwhisk:   \t0.2%\nheat:   \t0.16%\n--------\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Analysis for the Prompt: \"recipe for roasted vegetables | chop 1 /\"**\n**Top Predictions:**\n- Word \"2\" with a probability of 42.78%\n- Word \"4\" with a probability of 33.32%\n- Word \"3\" with a probability of 7.24%\n- Word \"8\" with a probability of 3.59%\n- Word \"1\" with a probability of 3.33%\n\n**Analysis for the Prompt: \"recipe for roasted vegetables | chop 1 / 4\"**\n**Top Predictions:**\n- Word \"-\" with a probability of 53.76%\n- Word \"cup\" with a probability of 19.65%\n- Word \"inch\" with a probability of 4.73%\n- Word \"cups\" with a probability of 4.3%\n- Word \"teaspoon\" with a probability of 2.38%\n\n**Interpretation:**\n- The model's predictions are influenced by the training data, and the likelihood percentages reflect the model's confidence in each word.\n- For the first prompt, the model predicts numerical values (\"2\", \"4\", \"3\", \"8\", \"1\") with relatively high probabilities, suggesting a continuation involving numerical quantities.\n- In the second prompt, the model predicts a dash (\"-\") with the highest probability, indicating a potential continuation in the form of a list or step in a recipe. Other predictions include units of measurement like \"cup,\" \"inch,\" \"cups,\" and \"teaspoon.\"\n","metadata":{}}]}